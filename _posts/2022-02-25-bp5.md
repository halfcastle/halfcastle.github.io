---
layout: post
title: ☆Blog Post 5☆
---

cat or dog

In this blog post, we will write a TensorFlow algorithm that distinguishes between pictures of cats and dogs. Though my 2-year-old nephew can already do this without much training, a computer is way less smart.

# 1. Load Packages and Obtain Data

First, as always, we have to import the relevant packages. Some of these won't come into use at a later stage.


```python
import os
import tensorflow as tf
from tensorflow.keras import utils 
from tensorflow.keras import datasets, layers, models, losses
import matplotlib.pyplot as plt
import numpy as np
```

Now we'll load the dataset of cat and dog images. We create a dataset for training and another for validation. The test dataset is formed by taking every 5th observation out of the validation dataset.

Our program takes 32 images at one time (thus the "batch size"). It also scales all the images to the same 160,160 size, which is necessary for our algorithm to work.


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
    68608000/68606236 [==============================] - 0s 0us/step
    68616192/68606236 [==============================] - 0s 0us/step
    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.


This block of code rapidly reads the data. 


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

### Working with datasets

Now that we have our data loaded, we'll create a nice visualization of two rows of cat/dogs, with 3 on each row.\
We will find 6 pictures from a set of 32 by using train_dataset.take(1).



```python
#fragment code in tutorial so that each applies to cats and dogs 
#(get 1 row of cats and 1 row of dogs)
names = ["cat", "dog"]
def visualization():
  plt.figure(figsize=(10, 10))
  k=1
  #counter to keep track of 6 pictures
  for images, labels in train_dataset.take(1):
    for i in range(len(labels)):
      #if cats
      if labels[i] == 0:
        if k<4:
          #select 3 pictures of dogs
          ax = plt.subplot(2,3,k)
          k+=1
          plt.imshow(images[i].numpy().astype("uint8"))
          plt.title(classnames[labels[i]])
          plt.axis("off")
    for i in range(len(labels)):
      if labels[i] == 1:
        if k<7:
          #select 3 pictures of dogs
          ax = plt.subplot(2,3,k)
          k+=1
          plt.imshow(images[i].numpy().astype("uint8"))
          plt.title(names[labels[i]])
          plt.axis("off")
```


```python
visualization()
```


    
![bp51.png](/images/bp51.png)
    


### Check Label Frequencies

In the images of our dataset, how many are cats and how many are dogs? \
The following line of code generates a labels iterator, which runs through the image labels in our data. If a label is 0, it is a cat; and if it's 1, it is a dog. 


```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```


```python
dogs = 0
cats = 0
for label in labels_iterator:
  if label==0:
    cats+=1
  else:
    dogs+=1
print(cats, dogs)
```

    1000 1000


From the results, we can see that we have an equal number of 1000 cats and 1000 dogs. Thus, the baseline accuracy would be around 0.5.

# 2. First Model!

Now it's time for the real thing. 

We will make a simple `tf.keras.Sequential` model with a few different layers: **Conv2D**, which creates convolution kernels according to user input;  **MaxPooling 2D** downsamples data dimensions, it's kind of like "summarizing" the data by sliding a window over your data and picking out only the largest element within the window; **Flatten**, as its name suggests, flattens the data to 1d in order to pass it through the final **Dense** layers, which forms the prediction; **Dropout** prevents overfitting by randomly setting input units to 0 during training.


```python
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(2) 
    ])
#compling
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
#fitting
history = model.fit(train_dataset,
                    epochs=20,
                    validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 38s 594ms/step - loss: 85.2573 - accuracy: 0.4740 - val_loss: 0.7054 - val_accuracy: 0.5458
    Epoch 2/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.6169 - accuracy: 0.6280 - val_loss: 0.7707 - val_accuracy: 0.5507
    Epoch 3/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.5165 - accuracy: 0.7195 - val_loss: 0.9236 - val_accuracy: 0.5594
    Epoch 4/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.3721 - accuracy: 0.8110 - val_loss: 1.3935 - val_accuracy: 0.5730
    Epoch 5/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.3132 - accuracy: 0.8555 - val_loss: 1.3153 - val_accuracy: 0.5606
    Epoch 6/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.2720 - accuracy: 0.8845 - val_loss: 1.1200 - val_accuracy: 0.5780
    Epoch 7/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.2155 - accuracy: 0.9085 - val_loss: 1.7291 - val_accuracy: 0.5866
    Epoch 8/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.1563 - accuracy: 0.9360 - val_loss: 1.9089 - val_accuracy: 0.5730
    Epoch 9/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.1220 - accuracy: 0.9495 - val_loss: 2.3335 - val_accuracy: 0.5990
    Epoch 10/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.1411 - accuracy: 0.9485 - val_loss: 2.0586 - val_accuracy: 0.5408
    Epoch 11/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.1302 - accuracy: 0.9580 - val_loss: 2.3951 - val_accuracy: 0.5532
    Epoch 12/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.1160 - accuracy: 0.9620 - val_loss: 2.9967 - val_accuracy: 0.5668
    Epoch 13/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.0889 - accuracy: 0.9645 - val_loss: 3.7136 - val_accuracy: 0.5755
    Epoch 14/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.0915 - accuracy: 0.9695 - val_loss: 2.9974 - val_accuracy: 0.5693
    Epoch 15/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.1007 - accuracy: 0.9655 - val_loss: 3.0144 - val_accuracy: 0.5545
    Epoch 16/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.0793 - accuracy: 0.9725 - val_loss: 3.7012 - val_accuracy: 0.5557
    Epoch 17/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.0763 - accuracy: 0.9765 - val_loss: 4.3287 - val_accuracy: 0.5866
    Epoch 18/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.1350 - accuracy: 0.9620 - val_loss: 3.1909 - val_accuracy: 0.5792
    Epoch 19/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.1584 - accuracy: 0.9560 - val_loss: 2.8023 - val_accuracy: 0.5681
    Epoch 20/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.0869 - accuracy: 0.9710 - val_loss: 3.7109 - val_accuracy: 0.5804


We will plot the training data to visualize how the model performed:


```python
plt.plot(history.history["accuracy"], label = "training", color = "violet")
plt.plot(history.history["val_accuracy"], label = "validation", color = "darkturquoise")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f52f751dd50>




   
![bp52.png](/images/bp52.png)
    


1. **For model1, the accuracy stabilized between 55% and 58% during the training.**\
2. Compared to a baseline of 0.5, it was ok.\
3. THere is some overfitting going on in model 1: the accuracy is quite different between the training and validation data.

# 3. Model With Data Augmentation

In this section, we will introduce data augmentation into our model. We can alter the rotation of the image, producing modified copies of the same image (rotated cats are still cats!). By doing so, our model can learn the "invariant features" (the cat-ness of a cat) of our images.

We will make two new layers: `RandomFlip()` and `RandomRotation()`. Their functions are true to their name.

Let's write the `RandomFlip()` layer first:


```python
flippy = tf.keras.Sequential([tf.keras.layers.RandomFlip('horizontal_and_vertical'),])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i+1)
    augmented_image = flippy(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![bp53.png](/images/bp53.png)
    


9 nice pictures of flippy cats are formed.

Next, we'll write the `RandomRotation()` layer


```python
rotaty= tf.keras.Sequential([tf.keras.layers.RandomRotation(0.2),])
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = rotaty(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![bp54.png](/images/bp54.png)
    


twisty cats...

now let's build model 2. It's essentially the same as model 1 except we added in our newly written flip and rotate layers.


```python
model2 = models.Sequential([
    layers.RandomFlip('horizontal_and_vertical'),
    layers.RandomRotation(0.2),
    #the layers below are our original layers
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(2) 

])
```


```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 65ms/step - loss: 135.6733 - accuracy: 0.5280 - val_loss: 0.7088 - val_accuracy: 0.5433
    Epoch 2/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.7106 - accuracy: 0.5525 - val_loss: 0.7080 - val_accuracy: 0.5458
    Epoch 3/20
    63/63 [==============================] - 4s 65ms/step - loss: 0.6875 - accuracy: 0.5595 - val_loss: 0.6896 - val_accuracy: 0.5792
    Epoch 4/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.6752 - accuracy: 0.5775 - val_loss: 0.6726 - val_accuracy: 0.6052
    Epoch 5/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.6834 - accuracy: 0.5785 - val_loss: 0.6671 - val_accuracy: 0.6139
    Epoch 6/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.6855 - accuracy: 0.5990 - val_loss: 0.6887 - val_accuracy: 0.5792
    Epoch 7/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.6827 - accuracy: 0.5890 - val_loss: 0.6894 - val_accuracy: 0.5953
    Epoch 8/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.6721 - accuracy: 0.6095 - val_loss: 0.6771 - val_accuracy: 0.5842
    Epoch 9/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.6762 - accuracy: 0.6000 - val_loss: 0.6650 - val_accuracy: 0.6064
    Epoch 10/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.6765 - accuracy: 0.5875 - val_loss: 0.6702 - val_accuracy: 0.6015
    Epoch 11/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.6578 - accuracy: 0.6025 - val_loss: 0.6603 - val_accuracy: 0.6040
    Epoch 12/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.6661 - accuracy: 0.5905 - val_loss: 0.6778 - val_accuracy: 0.6101
    Epoch 13/20
    63/63 [==============================] - 4s 65ms/step - loss: 0.6692 - accuracy: 0.6120 - val_loss: 0.6599 - val_accuracy: 0.6238
    Epoch 14/20
    63/63 [==============================] - 4s 65ms/step - loss: 0.6491 - accuracy: 0.6205 - val_loss: 0.6601 - val_accuracy: 0.6200
    Epoch 15/20
    63/63 [==============================] - 4s 65ms/step - loss: 0.6533 - accuracy: 0.6305 - val_loss: 0.6597 - val_accuracy: 0.6238
    Epoch 16/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.6700 - accuracy: 0.6050 - val_loss: 0.6519 - val_accuracy: 0.6126
    Epoch 17/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.6414 - accuracy: 0.6410 - val_loss: 0.6571 - val_accuracy: 0.6213
    Epoch 18/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.6545 - accuracy: 0.6145 - val_loss: 0.6563 - val_accuracy: 0.6250
    Epoch 19/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.6569 - accuracy: 0.6070 - val_loss: 0.6607 - val_accuracy: 0.6089
    Epoch 20/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.6500 - accuracy: 0.6350 - val_loss: 0.6748 - val_accuracy: 0.6027


Once again, plotting the visualizations for model performance


```python
plt.plot(history.history["accuracy"], label = "training", color = "violet")
plt.plot(history.history["val_accuracy"], label = "validation", color = "darkturquoise")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f52f2ad6bd0>




    
![bp55.png](/images/bp55.png)
    


1. **For model 2, the accuracy stabilized between 60% and 62% during the training.**\
2. Compared to the accuracy we got in model 1, it now increased by about 5%\
3. The accuracy of the training set vs the validation set is quite similar. There is little to no overfitting.

# 4. Data Preprocessing

Let's make our model even better by normalizing its RGB values. Traditionally, RGB values fall between 0-255, but we can normalize it between 0 to 1 or -1 to 1. By doing so, we can spend more of our training energy handling actual signal in the data and less energy having the weights adjust to the data scale.

The following code will create a preprocessing layer called preprocessor which can be slotted into the model pipeline.


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

Now we can add our preprocessor to our new model!


```python
model3 = models.Sequential([
    preprocessor,
    layers.RandomFlip('horizontal_and_vertical'),
    layers.RandomRotation(0.2),
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(2) 

])
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 8s 108ms/step - loss: 0.7506 - accuracy: 0.5220 - val_loss: 0.6910 - val_accuracy: 0.5470
    Epoch 2/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.6641 - accuracy: 0.5795 - val_loss: 0.6522 - val_accuracy: 0.6238
    Epoch 3/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.6495 - accuracy: 0.6175 - val_loss: 0.6317 - val_accuracy: 0.6696
    Epoch 4/20
    63/63 [==============================] - 4s 65ms/step - loss: 0.6343 - accuracy: 0.6440 - val_loss: 0.6144 - val_accuracy: 0.6522
    Epoch 5/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.6174 - accuracy: 0.6650 - val_loss: 0.6193 - val_accuracy: 0.6436
    Epoch 6/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.6030 - accuracy: 0.6735 - val_loss: 0.5803 - val_accuracy: 0.7067
    Epoch 7/20
    63/63 [==============================] - 4s 67ms/step - loss: 0.5932 - accuracy: 0.6730 - val_loss: 0.5767 - val_accuracy: 0.6918
    Epoch 8/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.5711 - accuracy: 0.6985 - val_loss: 0.5702 - val_accuracy: 0.7129
    Epoch 9/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.5670 - accuracy: 0.6960 - val_loss: 0.5635 - val_accuracy: 0.7265
    Epoch 10/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.5658 - accuracy: 0.7070 - val_loss: 0.5753 - val_accuracy: 0.7017
    Epoch 11/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.5589 - accuracy: 0.7135 - val_loss: 0.5395 - val_accuracy: 0.7339
    Epoch 12/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.5413 - accuracy: 0.7280 - val_loss: 0.5764 - val_accuracy: 0.7215
    Epoch 13/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.5502 - accuracy: 0.7115 - val_loss: 0.5606 - val_accuracy: 0.7067
    Epoch 14/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.5402 - accuracy: 0.7235 - val_loss: 0.5503 - val_accuracy: 0.7364
    Epoch 15/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.5287 - accuracy: 0.7295 - val_loss: 0.5439 - val_accuracy: 0.7389
    Epoch 16/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.5349 - accuracy: 0.7310 - val_loss: 0.5494 - val_accuracy: 0.7153
    Epoch 17/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.5237 - accuracy: 0.7295 - val_loss: 0.5552 - val_accuracy: 0.7129
    Epoch 18/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.5203 - accuracy: 0.7460 - val_loss: 0.5522 - val_accuracy: 0.7265
    Epoch 19/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.5114 - accuracy: 0.7505 - val_loss: 0.5356 - val_accuracy: 0.7438
    Epoch 20/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.4983 - accuracy: 0.7515 - val_loss: 0.5279 - val_accuracy: 0.7364


Once again, plotting the visualizations for model performance


```python
plt.plot(history.history["accuracy"], label = "training", color = "violet")
plt.plot(history.history["val_accuracy"], label = "validation", color = "darkturquoise")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f5394289d50>




    
![bp56.png](/images/bp56.png)
    


1. **For model 3, the accuracy stabilized between 65% and 73% during the training.**\
2. Model 3 has a much higher accuracy than both model 1 (55-58) and model 2 (60-62).\
3. Once again, the accuracy of the training set vs the validation set is quite similar. There is little to no overfitting.

# 5. Transfer Learning

We can also access a pre-existing model to further increase the accuracy of our model. In this section, we will access a pre-existing "base model" and incorporate it into a full model for our current task, then train that model.

The following code is provided by Dr Chodrow. It lets us download MobileNetV2 and configure it as a usable layer. 


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

Now it's time to construct our model. The code looks so much cleaner than before because the base_model_layer already has most of the things we need. When it comes to training models, it's sometimes ok to use other people's work. (...maybe?)


```python
model4 = models.Sequential([
    preprocessor,
    layers.RandomFlip('horizontal_and_vertical'),
    layers.RandomRotation(0.2),
    base_model_layer,
    layers.GlobalMaxPooling2D(),
    layers.Dropout(0.2),
    layers.Dense(2)
])
```

We can take a look at the model summary and see what's hidden inside base_model_layer!


```python
model4.summary()
```

    Model: "sequential_22"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model_1 (Functional)        (None, 160, 160, 3)       0         
                                                                     
     random_flip_19 (RandomFlip)  (None, 160, 160, 3)      0         
                                                                     
     random_rotation_19 (RandomR  (None, 160, 160, 3)      0         
     otation)                                                        
                                                                     
     model_4 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     global_max_pooling2d_5 (Glo  (None, 1280)             0         
     balMaxPooling2D)                                                
                                                                     
     dropout_33 (Dropout)        (None, 1280)              0         
                                                                     
     dense_40 (Dense)            (None, 2)                 2562      
                                                                     
    =================================================================
    Total params: 2,260,546
    Trainable params: 2,562
    Non-trainable params: 2,257,984
    _________________________________________________________________


We have 2562 trainable params-that is a lot! Our model is very likely to have high accuracy-let's try it out!


```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 9s 76ms/step - loss: 1.0453 - accuracy: 0.7460 - val_loss: 0.1648 - val_accuracy: 0.9493
    Epoch 2/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.5559 - accuracy: 0.8645 - val_loss: 0.0975 - val_accuracy: 0.9641
    Epoch 3/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.4448 - accuracy: 0.8830 - val_loss: 0.0898 - val_accuracy: 0.9715
    Epoch 4/20
    63/63 [==============================] - 4s 68ms/step - loss: 0.4373 - accuracy: 0.8865 - val_loss: 0.0751 - val_accuracy: 0.9752
    Epoch 5/20
    63/63 [==============================] - 5s 68ms/step - loss: 0.4107 - accuracy: 0.8910 - val_loss: 0.1783 - val_accuracy: 0.9431
    Epoch 6/20
    63/63 [==============================] - 5s 68ms/step - loss: 0.3609 - accuracy: 0.9045 - val_loss: 0.0746 - val_accuracy: 0.9790
    Epoch 7/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.3969 - accuracy: 0.8960 - val_loss: 0.0977 - val_accuracy: 0.9653
    Epoch 8/20
    63/63 [==============================] - 5s 68ms/step - loss: 0.3260 - accuracy: 0.9095 - val_loss: 0.0825 - val_accuracy: 0.9765
    Epoch 9/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.3810 - accuracy: 0.9010 - val_loss: 0.0872 - val_accuracy: 0.9691
    Epoch 10/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.3236 - accuracy: 0.9135 - val_loss: 0.0879 - val_accuracy: 0.9715
    Epoch 11/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.3255 - accuracy: 0.9040 - val_loss: 0.1223 - val_accuracy: 0.9616
    Epoch 12/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.3359 - accuracy: 0.9020 - val_loss: 0.0991 - val_accuracy: 0.9641
    Epoch 13/20
    63/63 [==============================] - 4s 65ms/step - loss: 0.3139 - accuracy: 0.9080 - val_loss: 0.0758 - val_accuracy: 0.9691
    Epoch 14/20
    63/63 [==============================] - 4s 65ms/step - loss: 0.3198 - accuracy: 0.9075 - val_loss: 0.0962 - val_accuracy: 0.9703
    Epoch 15/20
    63/63 [==============================] - 4s 67ms/step - loss: 0.2878 - accuracy: 0.9105 - val_loss: 0.0621 - val_accuracy: 0.9752
    Epoch 16/20
    63/63 [==============================] - 4s 65ms/step - loss: 0.2722 - accuracy: 0.9155 - val_loss: 0.0744 - val_accuracy: 0.9752
    Epoch 17/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.3540 - accuracy: 0.9040 - val_loss: 0.0664 - val_accuracy: 0.9752
    Epoch 18/20
    63/63 [==============================] - 4s 67ms/step - loss: 0.3071 - accuracy: 0.9060 - val_loss: 0.0679 - val_accuracy: 0.9752
    Epoch 19/20
    63/63 [==============================] - 5s 68ms/step - loss: 0.2902 - accuracy: 0.9145 - val_loss: 0.0745 - val_accuracy: 0.9740
    Epoch 20/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.2678 - accuracy: 0.9135 - val_loss: 0.1071 - val_accuracy: 0.9641


The highest we've got was around 97%.


```python
plt.plot(history.history["accuracy"], label = "training", color = "violet")
plt.plot(history.history["val_accuracy"], label = "validation", color = "darkturquoise")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f542e536950>




    
![bp57.png](/images/bp57.png)
    


1. **For model 4, the accuracy stabilized between 95% and 97% during the training.**\
2. Compared to all other models before, model 4 has a much higher accuracy. (lowest bound 22% higher than upper bound of model 3, our best performing model yet)
3. The validation accuracy stays above the training accuracy, thus it's safe to conclude that there isn't any overfitting. 

# 6. Score on Test Data

Finally, we can try out model 4 on our test dataset!


```python
model4.evaluate(test_dataset)
```

    6/6 [==============================] - 1s 46ms/step - loss: 0.1093 - accuracy: 0.9583





    [0.10929068177938461, 0.9583333134651184]



We achieved around 96% accuracy! This wasn't as high as I expected, but it's good enough nevertheless.
