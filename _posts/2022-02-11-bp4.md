## Spectral Clustering

In this blog post, I will write a spectral clustering algorithm to separate data points. 

By using spectral clustering, we can categorize our data according to relative distance between data points. We will first make a similarity matrix from our data, then calculate the eigenvalues of the Laplacian matrix, then yield labels that categorizes the points. 

We will first look at data that doesn't need spectral clustering.


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fe972893580>



    
![output_5_1.png](/images/output_5_1.png)



K-means is a very common way to achieve clustering task, which has good performance on circular-ish blobs like these:


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fe960c8b250>




    
![output_5_1.png](/images/output_5_1.png)
    


### Harder Clustering

Now we will see how kmeans works (or in this case, does not work) on data that is nicely clustered, but moon shaped.


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fe920030700>




    
![output_7_1.png](/images/output_7_1.png)
    


K-means doesn't work anymore, becaues it only looks for circular clusters


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fe960ca36d0>




    
![output_9_1.png](/images/output_9_1.png)    


With spectral clustering, however, we'll be able to separate the two chunks of data.

## Part A

First, we will create a similarity matrix (n, n) using epsilon = 0.4. We will determine if the distance between 2 datapoints is near enough (within 0.4). An entry `A[i,j]` where `X[i]` is within 0.4 with `X[j]` will be labeled as 1, while others will be 0. The diagonal entries `A[i,i]` will also be labeled 0. 


```python
epsilon = 0.4
# we will use pairwise_distances from the sklearn package! 
#it conveniently computes all the pairwise distances. yay!
from sklearn.metrics.pairwise import pairwise_distances
matrixy = pairwise_distances(X)
matrixy
```




    array([[0.        , 1.27292462, 1.33315598, ..., 1.9812102 , 1.68337039,
            1.94073324],
           [1.27292462, 0.        , 1.46325112, ..., 1.93729167, 1.68543003,
            1.91287315],
           [1.33315598, 1.46325112, 0.        , ..., 0.64857172, 0.35035968,
            0.60860868],
           ...,
           [1.9812102 , 1.93729167, 0.64857172, ..., 0.        , 0.30070415,
            0.04219636],
           [1.68337039, 1.68543003, 0.35035968, ..., 0.30070415, 0.        ,
            0.26255757],
           [1.94073324, 1.91287315, 0.60860868, ..., 0.04219636, 0.26255757,
            0.        ]])



Now let's create a matrix that looks like *matrixy* that is full of zeros, then fill the 1s in. We will use the np.zeros_like function to copy matrixy.


```python
A=np.zeros_like(matrixy)
A[matrixy<0.4]=1 #change all the values where matrixy values are <0.4
np.fill_diagonal(A,0) #fill all diagonal values with 0
A
```




    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 1., 0.],
           ...,
           [0., 0., 0., ..., 0., 1., 1.],
           [0., 0., 1., ..., 1., 0., 1.],
           [0., 0., 0., ..., 1., 1., 0.]])



## Part B

For part B, we will compute the binary norm cut objective of matrix A using the function

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right);.$$



C0 and C1 are the two clusters of our data points, and we assume that every data point is in either C0 or C1. We specify cluster membership by `y`: for example, `y[i]=1` means that point `i` belongs to cluster C1. 

The function contains two parts: the cut term, and the volume term. \
-The cut term, $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ measures the dissimilarity between our clusters. The smaller it is, the more dissimilar the clusters are.\ 
-The volume term, $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, is a measure of the size of the cluster. $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$).

Ultimately, a smaller binary norm cut objective indicates that we have 2 distinct clusters of data that have a relatively large size.\
Which is a good thing.

#### B.1 The Cut Term
The cut term $$\mathbf{cut}(C_0, C_1)$$ is the number of nonzero entries in $$\mathbf{A}$$ that relate points in cluster $$C_0$$ to points in cluster $$C_1$$. The smaller it is, the less number of related entries, and the more dissimilar the clusters. \
We will write a cut(A,y) function that takes in the data points and the label and then computes the cut term.


```python
def cut(A, y):
    cut=0
    for i in range(len(y)):
        for j in range(len(y)):
            if y[i]==0 and y[j]==1:
                cut=cut + A[i,j]
    return cut
```

Next, we will compute the cut objective for the true clusters `y`, then compute the same thing for a "fake cluster" of random labels in the same size. \
We will find that the cut objective for the true lables is much smaller than the cut objective for the random labels.


```python
cut(A, y)
```




    13.0




```python
randomvector = np.random.randint(2, size=n)
cut(A, randomvector)
```




    1150.0



#### B.2 The Volume Term 

We will now compute the volume term, which is a measure of cluster size. If we choose cluster $C_0$ to be small, then $\mathbf{vol}(C_0)$ will be small and $$\frac{1}{\mathbf{vol}(C_0)}$$ will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters $$C_0$$ and $$C_1$$ such that:

1. There are relatively few entries of $\mathbf{A}$ that join $$C_0$$ and $$C_1$$. 
2. Neither $$C_0$$ and $$C_1$$ are too small. 

The vols function computes the volumes of $$C_0$$ and $$C_1$$ and returns them as a tuple. 
The normcut function uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 


```python
def vols(A, y):
    v0 = A[y==0].sum()
    v1 = A[y==1].sum()
    return (v0, v1)
vols(A,y)
```




    (2299.0, 2217.0)




```python
def normcut(A, y):
    (v0, v1) = vols(A, y)
    return cut(A, y)*(1/v0+1/v1)
```

We'll try the same comparison between true and random labels with the normcut function.


```python
trueNC = normcut(A, y)
trueNC
```




    0.011518412331615225




```python
fakeNC = normcut(A, randomvector)
fakeNC
```




    1.0240023597759158



As indicated by the output, we can see that the normcut objective for the true label y is a lot smaller than the one for the random fake labels. Noice.

## Part C

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $A$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

Note that the signs of  the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$: if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$. 

Here's how we compute the value of z using the trick mentioned above:


```python
def transform(A, y):
    (v0, v1) = vols(A, y)
    z = np.zeros(len(y))
    #masky
    z[y==0]=1/v0
    z[y==1]=-1/v1
    return z
```

Now we're going to show that the two sides of the following equation is equal by computing each side separately and checking that they are equal. 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  


```python
D = np.zeros_like(A)
np.fill_diagonal(D, A.sum(axis=1))
z = transform(A,y)
#right side of equation
na = (z.T@(D-A)@z)/(z.T@D@z) 

np.isclose(na, trueNC)
#na
```




    True



Yay!

## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. The orth and orth_obj functions below are defined by Prof. Chodrow: 


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e
def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Does our optimization work? Let's try plotting our moon data again.


```python
from scipy.optimize import minimize
zz = minimize(orth_obj, x0=e)
z_min = zz.x
z_min[z_min>=0] = 1
z_min[z_min<0] = 0
```


```python
plt.scatter(X[:,0], X[:,1], c = z_min)
```




    <matplotlib.collections.PathCollection at 0x7fe972df5b50>




    
![output_38_1.png](/images/output_38_1.png)    


## Part F

Explicitly optimizing the orthogonal objective is way too slow to be practical. In this section, we are going to investigate the spectral part of spectral clustering by using eigenvalues and eigenvectors of matrices.

**Here are some math stuff from the tutorial of this blog post that I think explains the concept better than I can:**\
Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $\mathbf{z}$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

$$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, so the vector $$\mathbf{z}$$ that we want is the eigenvector with the second smallest eigenvalue. 

To find the eigenvector with the second smallest eigenvalue `z_eig`, we will construct the Laplacian matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$ of the similarity matrix $$\mathbf{A}$$. 


```python
L = np.linalg.inv(D)@(D-A)  #Constructing the Laplacin matrix L = D^-1(D-A)
Lam, U = np.linalg.eig(L)  #Lam are eigenvalues, U are eigenvectors
ix = Lam.argsort() #sorts indices
Lam, U = Lam[ix], U[:, ix] 
z_eig = U[:,1]
#finds the eigenvector with the second smallest eigenvalue 
```

Now, we will plot the graph again!


```python
plt.scatter(X[:,0], X[:,1], c = z_eig < 0)
```




    <matplotlib.collections.PathCollection at 0x7fe9200d0640>




    
![output_42_1.png](/images/output_42_1.png)    


Looks pretty nice!

## Part G


In this section, I will write a function that collectively uses the functions we have written in the previous sections to produce binary labels that categorize the data points into either group 0 or group 1. spectral_clustering takes in 2 variables: input data `X` and the distance threshold `epsilon`. 

The function does the following:
1. Constructs the similarity matrix and the Laplacian matrix.
2. Calculates the eigenvector that has the 2nd smallest eigenvalue.
3. Returns binary labels based on the eigenvector. 


```python
def spectral_clustering(X, epsilon):
    
    #construct similarity matrix
    matrixy = pairwise_distances(X)
    A=np.zeros_like(matrixy)
    A[matrixy<0.4]=1 
    np.fill_diagonal(A,0) 
    
    #construct Laplacian matrix
    D = np.zeros_like(A)
    np.fill_diagonal(D, A.sum(axis=1))
    L = np.linalg.inv(D)@(D-A) 
    
    #compute eigenvector and 2nd smallest eigenvalue
    Lam, U = np.linalg.eig(L) 
    U = U[:, Lam.argsort()] 
    z_eig = U[:,1]

    # yield binary labels based on the eigenvector
    label = np.where(z_eig < 0, 1, 0)
    
    return label

#This function has exactly 10 functional lines. I tried.
```


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```




    <matplotlib.collections.PathCollection at 0x7fe9184c5fd0>




    
![output_46_1.png](/images/output_46_1.png)    


## Part H

It is time to release the freshly baked spectral clustering function into the wild. In this section, I will demonstrate how my function works by generating different data sets using `make_moons`. I will also run some experiments to see what happens if the `noise` and `sample size`.

**At noise = 0.05**, everything is perfect.


```python
X, y = datasets.make_moons(n_samples=300, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering2(X, epsilon))
```




    <matplotlib.collections.PathCollection at 0x7fe918376e20>




    
![output_49_1.png](/images/output_49_1.png)    


**At noise = 0.1**, things still look pretty good.


```python
X, y = datasets.make_moons(n_samples=300, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering2(X, epsilon))
```




    <matplotlib.collections.PathCollection at 0x7fe961779eb0>




    
![png](output_51_1.png)
    


**At noise = 0.15**, things start to look weird.


```python
X, y = datasets.make_moons(n_samples=300, shuffle=True, noise=0.15, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering2(X, epsilon))
```




    <matplotlib.collections.PathCollection at 0x7fe9615cd9a0>




    
![output_53_1.png](/images/output_53_1.png)    


As shown in the above picture, as noise increases, the dots become more spread out. Sometimes two points that belong in different clusters are also very close to each other, so the function might be mistaking those as belonging to one cluster.

Nothing weird happens if we **increase the sample size to 1000**, though.


```python
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering2(X, epsilon))
```




    <matplotlib.collections.PathCollection at 0x7fe9183b7790>




    
![output_56_1.png](/images/output_56_1.png)    


## Part I

In the final section (yay, finally!), we'll try the function on a different dataset.


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fe92480b220>




    
![output_58_1.png](/images/output_58_1.png)    


k-means also doesn't work on this one. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fe92485eac0>




    
![output_60_1.png](/images/output_60_1.png)    


spectral_clustering to the rescue!


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
#here, epsilon is 0.4
```




    <matplotlib.collections.PathCollection at 0x7fe92475afa0>




    
![output_62_1.png](/images/output_62_1.png)

    


What if we change the value for epsilon? 


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.6))
```




    <matplotlib.collections.PathCollection at 0x7fe91870ef70>




    
![output_64_1.png](/output_64_1.png)
    


As shown in the graph, things start to look weird. There should be a optimal value range for epsilon so that our function works.

This is the end of the blog post. Yay.


```python

```
